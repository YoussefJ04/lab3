











# Change to path on your local machine.
data_path = r"D:\DataEngineering\lab3\assignment\data"






!pip install -U numpy pandas pyarrow matplotlib scipy
import sys, subprocess
try:
    import psutil  # noqa: F401
except Exception:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "psutil"])
print("psutil is installed.")


from IPython.core.magic import register_cell_magic
import time, os, platform

# Try to import optional modules
try:
    import psutil
except Exception:
    psutil = None

try:
    import resource  # not available on Windows
except Exception:
    resource = None


def _rss_bytes():
    """Resident Set Size in bytes (cross-platform via psutil if available)."""
    if psutil is not None:
        return psutil.Process(os.getpid()).memory_info().rss
    # Fallback: unknown RSS → 0 
    return 0


def _peak_bytes():
    """
    Best-effort peak memory in bytes.
    - Windows: psutil peak working set (peak_wset)
    - Linux:   resource.ru_maxrss (KB → bytes)
    - macOS:   resource.ru_maxrss (bytes)
    Fallback to current RSS if unavailable.
    """
    sysname = platform.system()

    # Windows path: use psutil peak_wset if present
    if sysname == "Windows" and psutil is not None:
        mi = psutil.Process(os.getpid()).memory_info()
        peak = getattr(mi, "peak_wset", None)  # should be available on Windows
        if peak is not None:
            return int(peak)
        return int(mi.rss)

    # POSIX path: resource may be available
    if resource is not None:
        try:
            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes
            if sysname == "Linux":
                return int(ru) * 1024
            else:
                return int(ru)
        except Exception:
            pass

    # Last resort
    return _rss_bytes()


@register_cell_magic
def timemem(line, cell):
    """
    Measure wall time and memory around the execution of this cell.

        %%timemem
        <your code>

    Notes:
    - RSS = resident memory after the cell.
    - Peak is OS-dependent (see _peak_bytes docstring).
    """
    ip = get_ipython()

    rss_before  = _rss_bytes()
    peak_before = _peak_bytes()
    t0 = time.perf_counter()

    # Execute the cell body
    result = ip.run_cell(cell)

    t1 = time.perf_counter()
    rss_after  = _rss_bytes()
    peak_after = _peak_bytes()

    wall = t1 - t0
    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)
    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)

    print("======================================")
    print(f"Wall time: {wall:.3f} s")
    print(f"RSS Δ: {rss_delta_mb:+.2f} MB")
    print(f"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)")
    print("======================================")

    return result





import os, sys
import pyspark
from pyspark.find_spark_home import _find_spark_home
from pyspark.sql import SparkSession


spark_home = _find_spark_home()
os.environ["SPARK_HOME"] = spark_home

spark_bin = os.path.join(spark_home, "bin")
if spark_bin not in os.environ.get("PATH", ""):
    os.environ["PATH"] = spark_bin + ";" + os.environ.get("PATH", "")

py = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = py
os.environ["PYSPARK_PYTHON"] = py

print("PYTHON =", py)
print("SPARK_HOME =", os.environ["SPARK_HOME"])
print("spark-submit exists =", os.path.exists(os.path.join(spark_bin, "spark-submit.cmd")))

try:
    spark.stop()
except:
    pass

spark = (
    SparkSession.builder
    .appName("A3")
    .master("local[*]")
    .config("spark.driver.memory", "8g")
    .config("spark.sql.shuffle.partitions", "400")
    .config("spark.sql.adaptive.enabled", "true")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("ERROR")
sc = spark.sparkContext
print("Spark:", spark.version)









# Note that you should have defined data_path above

events_df   = spark.read.parquet(os.path.join(data_path, "retail_dw_20250826_events"))
products_df = spark.read.parquet(os.path.join(data_path, "retail_dw_20250826_products"))
brands_df   = spark.read.parquet(os.path.join(data_path, "retail_dw_20250826_brands"))

events_df.printSchema()
products_df.printSchema()
brands_df.printSchema()





print(f"Number of rows in events   table: {events_df.count()}")
print(f"Number of rows in products table: {products_df.count()}")
print(f"Number of rows in brands   table: {brands_df.count()}")





events_df.createOrReplaceTempView("events")
products_df.createOrReplaceTempView("products")
brands_df.createOrReplaceTempView("brands")

spark.sql('select count(*) from events').show()
spark.sql('select count(*) from products').show()
spark.sql('select count(*) from brands').show()

















%%timemem
# codecell_31a (keep this id for tracking purposes)

# Write your SQL below
sql_query = """
SELECT
    session_id,
    price
FROM events
WHERE event_type = 'purchase'
  AND session_id = '789d3699-028e-4367-b515-b82e2cb5225f'
"""


results = spark.sql(sql_query)

results.show()





%%timemem
# codecell_31b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.
from pyspark.sql import functions as F

results_df = (
    events_df
    .filter(F.col("event_type") == "purchase")
    .filter(F.col("session_id") == "789d3699-028e-4367-b515-b82e2cb5225f")
    .select("session_id", "price")
)


results_df.show()











%%timemem
# codecell_32a (keep this id for tracking purposes)

# Write your SQL below
sql_query = """
SELECT COUNT(*) AS nb_products_sold
FROM events e
JOIN products p ON e.product_key = p.product_key
JOIN brands b   ON p.brand_key = b.brand_key
WHERE e.event_type = 'purchase'
  AND lower(b.brand_code) = 'sokolov'
"""
results = spark.sql(sql_query)
results.show()






%%timemem
# codecell_32b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.
from pyspark.sql import functions as F

e = events_df.alias("e")
p = products_df.alias("p")
b = brands_df.alias("b")

results_df = (
    e
    .filter(F.col("e.event_type") == "purchase")
    .join(p, on=F.col("e.product_key") == F.col("p.product_key"), how="inner")
    .join(b, on=F.col("p.brand_key") == F.col("b.brand_key"), how="inner")
    .filter(F.lower(F.col("b.brand_code")) == "sokolov")
    .select(F.count("*").alias("nb_products_sold"))
)

results_df.show()











%%timemem
# codecell_33a (keep this id for tracking purposes)

# Write your SQL below
sql_query = """
SELECT ROUND(AVG(e.price), 2) AS avg_purchase_price
FROM events e
JOIN products p ON e.product_key = p.product_key
JOIN brands  b ON p.brand_key = b.brand_key
WHERE e.event_type = 'purchase'
  AND e.price IS NOT NULL
  AND LOWER(b.brand_desc) LIKE '%febest%'
"""
results = spark.sql(sql_query)
results.show()






%%timemem
from pyspark.sql import functions as F

e = events_df.alias("e")
p = products_df.alias("p")
b = brands_df.alias("b")

base = (
    e.filter(F.col("e.event_type") == "purchase")
     .filter(F.col("e.price").isNotNull())
     .join(p, F.col("e.product_key") == F.col("p.product_key"), "inner")
     .join(b, F.col("p.brand_key") == F.col("b.brand_key"), "inner")
)

results_df = (
    base
    .filter(F.lower(F.col("b.brand_desc")).contains("febest"))   # <- IMPORTANT
    .select(F.round(F.avg("e.price"), 2).alias("avg_purchase_price"))
)

results_df.show()













%%timemem
# codecell_34a (keep this id for tracking purposes)

# Write your SQL below
sql_query = """
SELECT ROUND(AVG(nb_events), 2) AS avg_events_per_user
FROM (
    SELECT user_key, COUNT(*) AS nb_events
    FROM events
    GROUP BY user_key
) t
"""
results = spark.sql(sql_query)
results.show()






%%timemem
# codecell_34b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.

from pyspark.sql import functions as F

results_df = (
    events_df
    .groupBy("user_key")
    .agg(F.count("*").alias("nb_events"))
    .agg(F.round(F.avg("nb_events"), 2).alias("avg_events_per_user"))
)

results_df.show()











%%timemem
# codecell_35a (keep this id for tracking purposes)

# Write your SQL below
sql_query = """
SELECT
    p.product_name,
    p.brand_code,
    ROUND(SUM(e.price), 2) AS revenue
FROM events e
JOIN products p
    ON e.product_key = p.product_key
WHERE e.event_type = 'purchase'
GROUP BY p.product_name, p.brand_code
ORDER BY revenue DESC
LIMIT 10
"""



results = spark.sql(sql_query)

results.show()





%%timemem
# codecell_35b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.

from pyspark.sql import functions as F

results_df = (
    events_df
    .filter(F.col("event_type") == "purchase")
    .join(products_df, on="product_key", how="inner")
    .groupBy("product_name", "brand_code")
    .agg(F.round(F.sum("price"), 2).alias("revenue"))
    .orderBy(F.col("revenue").desc())
    .limit(10)
)



results_df.show(truncate=False)











%%timemem
# codecell_36a (keep this id for tracking purposes)

# Write your SQL below
sql_query = f"""

SELECT
  hour(event_time) AS hour,
  COUNT(*) AS count
FROM events
GROUP BY hour(event_time)
ORDER BY hour


"""

results = spark.sql(sql_query)

results.show(24)





%%timemem
# codecell_36b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.

from pyspark.sql import functions as F

events_by_hour_df = (
    events_df
    .withColumn("hour", F.hour("event_time"))
    .groupBy("hour")
    .count()
    .orderBy("hour")
)

events_by_hour_df.show(24)









%%timemem
# codecell_36c (keep this id for tracking purposes)

import matplotlib.pyplot as plt

events_by_hour_pdf = events_by_hour_df.toPandas()

# TODO: Write your code below, but do not remove any lines already in this cell.

plt.figure(figsize=(10, 4))
plt.plot(events_by_hour_pdf["hour"], events_by_hour_pdf["count"])
plt.xlabel("Hour")
plt.ylabel("Number of events")
plt.title("Events per hour")
plt.xticks(range(0, 24))
plt.grid(True)


plt.show()











%%timemem
# codecell_37a (keep this id for tracking purposes)

# Write your SQL below
sql_query = f"""

SELECT
  b.brand_code AS brand_code,
  ROUND(AVG(e.price), 2) AS avg_price
FROM events e
JOIN brands b
  ON e.brand_key = b.brand_key
WHERE e.event_type = 'purchase'
GROUP BY b.brand_code
HAVING AVG(e.price) > 10000
ORDER BY avg_price DESC


"""

results = spark.sql(sql_query)

results.show()





%%timemem
# codecell_37b (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.

from pyspark.sql import functions as F

avg_price_by_brand_df = (
    events_df
    .filter(F.col("event_type") == "purchase")
    .join(brands_df.select("brand_key", "brand_code"), on="brand_key", how="inner")
    .groupBy("brand_code")
    .agg(F.round(F.avg("price"), 2).alias("avg_price"))
    .filter(F.col("avg_price") > 10000)
    .orderBy(F.col("avg_price").desc())
)

avg_price_by_brand_df.show()











%%timemem
# codecell_37c (keep this id for tracking purposes)

import matplotlib.pyplot as plt

avg_price_by_brand_pdf = avg_price_by_brand_df.toPandas()

# TODO: Write your code below, but do not remove any lines already in this cell.

plt.figure(figsize=(10, 4))
plt.bar(avg_price_by_brand_pdf["brand_code"], avg_price_by_brand_pdf["avg_price"])
plt.xlabel("Brand")
plt.ylabel("Average purchase price")
plt.title("Average purchase price by brand (avg > 10000)")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()

plt.show()








# Get RDDs directly from DataFrames (with required repartitions)
# type: RDD[Row]
events_rdd   = events_df.rdd.repartition(1000)
products_rdd = products_df.rdd.repartition(100)
brands_rdd   = brands_df.rdd.repartition(100)





from pyspark.sql import Row











filtered_events_df = (
    events_df
        .filter((F.col("event_type") == "purchase") & F.col("price").isNotNull())
        .join(brands_df, on="brand_key")
)

filtered_events_df.count()

print(f"Number of rows in events          table: {events_df.count()}")
print(f"Number of rows in filtered events table: {filtered_events_df.count()}")

filtered_events_rdd = filtered_events_df.rdd








%%timemem
# codecell_5x1 (keep this id for tracking purposes)

# TODO: Write your code below, but do not remove any lines already in this cell.


"""
Version 1 groupByKey()

Objectif:
- Implémenter le calcul de la moyenne du prix par marque en utilisant groupByKey(),

Problème :
- groupByKey() rassemble toutes les valeurs associées à une clé (ici tous les prix
  d’une marque) avant le calcul.
- Sur un jeu de données volumineux, cette approche peut entraîner une forte
  consommation mémoire et provoquer l’échec de l’exécution en environnement local.
- Cette version est je pense correcte d’un point de vue algorithmique.

Conclusion :
- Je vais laisser cette implémentation qui aurait marcher sans ces problèmes.
- Une version plus efficace et adaptée aux grands volumes de données est proposée
  après avec reduceByKey().
"""



pairs = filtered_events_rdd.map(lambda r: (r["brand_code"], float(r["price"])))

grouped = pairs.groupByKey()

def avg_iter(prices_it):
    s = 0.0
    c = 0
    for x in prices_it:
        s += float(x)
        c += 1
    return (s / c) if c else None

avg_by_brand = (
    grouped
    .mapValues(avg_iter)
    .filter(lambda kv: kv[1] is not None)
    .mapValues(lambda avg: round(avg, 2))
)

average_revenue_per_brand_v1 = avg_by_brand.sortBy(
    lambda kv: (-kv[1], kv[0])
)

average_revenue_per_brand_v1.take(10)











%%timemem
# average purchase price by brand (v3: emit (sum, count) and aggregate early)

# 1) events -> (brand, (price, 1))
#    ⚠️ adapte les index/keys selon la structure de tes events
#    Exemple si event est un dict:
#      brand = e["brand"]
#      price = float(e["purchase_price"])
#    Exemple si event est un tuple:
#      brand = e[<brand_idx>]
#      price = float(e[<price_idx>])

def to_brand_sum_count(e):
    brand = e["brand"]                 # <-- adapte ici
    price = float(e["purchase_price"]) # <-- adapte ici
    return (brand, (price, 1))

average_revenue_per_brand_v3 = (
    filtered_events_rdd
    .map(to_brand_sum_count)
    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))   # (sum, count)
    .mapValues(lambda sc: round(sc[0] / sc[1], 2))          # avg arrondi
    .sortBy(lambda kv: kv[1], ascending=False)              # tri décroissant sur avg
)



import sys
print(sys.version)












spark.sql("""

SELECT * FROM brands b
JOIN products p ON p.brand_key = b.brand_key
WHERE b.brand_key = '423'

""").show()








# codecell_61a (keep this id for tracking purposes)

def shuffle_join(R, S, keyR, keyS):








%%timemem

shuffle_join_rdd = shuffle_join(brands_rdd, products_rdd, "brand_key", "brand_key")
shuffle_join_rdd.count()





shuffle_join_results_rdd = shuffle_join_rdd.filter(lambda row: row["brand_key"] == 423)
shuffle_join_results_rdd.count()





df = spark.createDataFrame(shuffle_join_results_rdd.collect())
df.show()











# codecell_62a (keep this id for tracking purposes)

def replicated_hash_join(R, S, keyR, keyS):








%%timemem

replicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, "brand_key", "brand_key")
replicated_hash_join_rdd.count()





replicated_hash_join_results_rdd = replicated_hash_join_rdd.filter(lambda row: row["brand_key"] == 423)
replicated_hash_join_results_rdd.count()





df = spark.createDataFrame(replicated_hash_join_results_rdd.collect())
df.show()














%%timemem

shuffle_join_rdd = shuffle_join(brands_rdd, products_rdd, "brand_key", "brand_key").filter(lambda row: row["brand_key"] == 423)
shuffle_join_rdd.count()





%%timemem

replicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, "brand_key", "brand_key").filter(lambda row: row["brand_key"] == 423)
replicated_hash_join_rdd.count()





%%timemem

replicated_hash_join_rdd = replicated_hash_join(products_rdd, brands_rdd, "brand_key", "brand_key").filter(lambda row: row["brand_key"] == 423)
replicated_hash_join_rdd.count()

















%%timemem
spark.stop()








if spark is not None:
    a = spark.sparkContext.parallelize([1,2,3,4])
    # write some code here to exercise your rdd_mean functions
    left = spark.sparkContext.parallelize([(1,'A'), (2,'B'), (3,'C')])
    right = spark.sparkContext.parallelize([(1,10), (2,20)])
    # write some code here to exercise your join functions
    pass




