{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6723da56",
   "metadata": {},
   "source": [
    "# DE1 — Lab 3: Physical Representations and Batch II Costs\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "Execute all cells. Capture plans and Spark UI evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f70000",
   "metadata": {},
   "source": [
    "## 0. Setup and explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1234c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"de1-lab3\")\n",
    "    # ✅ FIX Windows / Hadoop Native\n",
    "    .config(\n",
    "        \"spark.driver.extraJavaOptions\",\n",
    "        \"-Dorg.apache.hadoop.util.NativeCodeLoader.disable=true\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.executor.extraJavaOptions\",\n",
    "        \"-Dorg.apache.hadoop.util.NativeCodeLoader.disable=true\"\n",
    "    )\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "clicks_schema = T.StructType([\n",
    "    T.StructField(\"prev_title\", T.StringType(), True),\n",
    "    T.StructField(\"curr_title\", T.StringType(), True),\n",
    "    T.StructField(\"type\", T.StringType(), True),\n",
    "    T.StructField(\"n\", T.IntegerType(), True),\n",
    "    T.StructField(\"ts\", T.TimestampType(), True),\n",
    "])\n",
    "\n",
    "dim_schema = T.StructType([\n",
    "    T.StructField(\"curr_title\", T.StringType(), True),\n",
    "    T.StructField(\"curr_category\", T.StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5ecd6",
   "metadata": {},
   "source": [
    "## 1. Ingest monthly CSVs (row format baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"data/\"\n",
    "paths = [f\"{base}lab3_clicks_2025-05.csv\", f\"{base}lab3_clicks_2025-06.csv\", f\"{base}lab3_clicks_2025-07.csv\"]\n",
    "row_df = (spark.read.schema(clicks_schema).option(\"header\",\"true\").csv(paths)\n",
    "            .withColumn(\"year\", F.year(\"ts\")).withColumn(\"month\", F.month(\"ts\")))\n",
    "row_df.cache()\n",
    "print(\"Rows:\", row_df.count())\n",
    "row_df.printSchema()\n",
    "row_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf81861",
   "metadata": {},
   "source": [
    "### Evidence: row representation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87f6279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- TakeOrderedAndProject (10)\n",
      "   +- HashAggregate (9)\n",
      "      +- Exchange (8)\n",
      "         +- HashAggregate (7)\n",
      "            +- Project (6)\n",
      "               +- Filter (5)\n",
      "                  +- InMemoryTableScan (1)\n",
      "                        +- InMemoryRelation (2)\n",
      "                              +- * Project (4)\n",
      "                                 +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [6]: [curr_title#3261, month#3267, n#3263, prev_title#3260, type#3262, year#3266]\n",
      "Arguments: [curr_title#3261, month#3267, n#3263, prev_title#3260, type#3262, year#3266], [isnotnull(type#3262), (type#3262 = link)]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [prev_title#3260, curr_title#3261, type#3262, n#3263, ts#3264, year#3266, month#3267], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [5]: [prev_title#3260, curr_title#3261, type#3262, n#3263, ts#3264]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/D:/DataEngineering/lab3/labpractice/data/lab3_clicks_2025-05.csv, ... 2 entries]\n",
      "ReadSchema: struct<prev_title:string,curr_title:string,type:string,n:int,ts:timestamp>\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [7]: [prev_title#3260, curr_title#3261, type#3262, n#3263, ts#3264, year(cast(ts#3264 as date)) AS year#3266, month(cast(ts#3264 as date)) AS month#3267]\n",
      "Input [5]: [prev_title#3260, curr_title#3261, type#3262, n#3263, ts#3264]\n",
      "\n",
      "(5) Filter\n",
      "Input [6]: [curr_title#3261, month#3267, n#3263, prev_title#3260, type#3262, year#3266]\n",
      "Condition : (isnotnull(type#3262) AND (type#3262 = link))\n",
      "\n",
      "(6) Project\n",
      "Output [5]: [prev_title#3260, curr_title#3261, n#3263, year#3266, month#3267]\n",
      "Input [6]: [curr_title#3261, month#3267, n#3263, prev_title#3260, type#3262, year#3266]\n",
      "\n",
      "(7) HashAggregate\n",
      "Input [5]: [prev_title#3260, curr_title#3261, n#3263, year#3266, month#3267]\n",
      "Keys [4]: [year#3266, month#3267, prev_title#3260, curr_title#3261]\n",
      "Functions [1]: [partial_sum(n#3263)]\n",
      "Aggregate Attributes [1]: [sum#3770L]\n",
      "Results [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, sum#3771L]\n",
      "\n",
      "(8) Exchange\n",
      "Input [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, sum#3771L]\n",
      "Arguments: hashpartitioning(year#3266, month#3267, prev_title#3260, curr_title#3261, 200), ENSURE_REQUIREMENTS, [plan_id=630]\n",
      "\n",
      "(9) HashAggregate\n",
      "Input [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, sum#3771L]\n",
      "Keys [4]: [year#3266, month#3267, prev_title#3260, curr_title#3261]\n",
      "Functions [1]: [sum(n#3263)]\n",
      "Aggregate Attributes [1]: [sum(n#3263)#3664L]\n",
      "Results [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, sum(n#3263)#3664L AS n#3656L]\n",
      "\n",
      "(10) TakeOrderedAndProject\n",
      "Input [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, n#3656L]\n",
      "Arguments: 50, [n#3656L DESC NULLS LAST], [year#3266, month#3267, prev_title#3260, curr_title#3261, n#3656L]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [5]: [year#3266, month#3267, prev_title#3260, curr_title#3261, n#3656L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved proof/plan_row.txt\n"
     ]
    }
   ],
   "source": [
    "# Query Q1: top transitions per month for 'link'\n",
    "q1_row = (row_df.filter(F.col(\"type\")==\"link\")\n",
    "           .groupBy(\"year\",\"month\",\"prev_title\",\"curr_title\")\n",
    "           .agg(F.sum(\"n\").alias(\"n\"))\n",
    "           .orderBy(F.desc(\"n\"))\n",
    "           .limit(50))\n",
    "q1_row.explain(\"formatted\")\n",
    "\n",
    "import pathlib, datetime as _dt\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_row.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.datetime.now())+\"\\n\")\n",
    "    f.write(q1_row._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_row.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde014ce",
   "metadata": {},
   "source": [
    "## 2. Column representation: Parquet with partitioning and optional sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1254ef78",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o499.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m col_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataEngineering\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlab3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlabpractice\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlab3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcolumnar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Write columnar\u001b[39;00m\n\u001b[0;32m      4\u001b[0m (row_df\n\u001b[0;32m      5\u001b[0m  \u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m  \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m  \u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/clicks_parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Re‑read columnar for fair comparison\u001b[39;00m\n\u001b[0;32m     10\u001b[0m col_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mschema(clicks_schema\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/clicks_parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   2001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o499.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "col_base = r\"D:\\DataEngineering\\lab3\\labpractice\\outputs\\lab3\\columnar\"\n",
    "\n",
    "# Write columnar\n",
    "(\n",
    "    row_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .parquet(f\"{col_base}/clicks_parquet\")\n",
    ")\n",
    "\n",
    "# Re-read columnar for fair comparison\n",
    "col_df = (\n",
    "    spark.read\n",
    "    .schema(clicks_schema.add(\"year\", \"int\").add(\"month\", \"int\"))\n",
    "    .parquet(f\"{col_base}/clicks_parquet\")\n",
    ")\n",
    "\n",
    "col_df.cache()\n",
    "print(\"Columnar rows:\", col_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cfe86",
   "metadata": {},
   "source": [
    "### Evidence: column representation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a9e8d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q1_col \u001b[38;5;241m=\u001b[39m (col_df\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m            \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_title\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurr_title\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m            \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m            \u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mdesc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m            \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m50\u001b[39m))\n\u001b[0;32m      6\u001b[0m q1_col\u001b[38;5;241m.\u001b[39mexplain(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformatted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproof/plan_column.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'col_df' is not defined"
     ]
    }
   ],
   "source": [
    "q1_col = (col_df.filter(F.col(\"type\")==\"link\")\n",
    "           .groupBy(\"year\",\"month\",\"prev_title\",\"curr_title\")\n",
    "           .agg(F.sum(\"n\").alias(\"n\"))\n",
    "           .orderBy(F.desc(\"n\"))\n",
    "           .limit(50))\n",
    "q1_col.explain(\"formatted\")\n",
    "with open(\"proof/plan_column.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(q1_col._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_column.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001d4df",
   "metadata": {},
   "source": [
    "## 3. Join strategy: normal vs broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = spark.read.schema(dim_schema).option(\"header\",\"true\").csv(\"data/lab3_dim_curr_category.csv\")\n",
    "# Non‑broadcast join\n",
    "j1 = (col_df.join(dim, \"curr_title\", \"left\")\n",
    "      .groupBy(\"curr_category\")\n",
    "      .agg(F.sum(\"n\").alias(\"total_n\"))\n",
    "      .orderBy(F.desc(\"total_n\")))\n",
    "j1.explain(\"formatted\")\n",
    "\n",
    "# Broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "j2 = (col_df.join(broadcast(dim), \"curr_title\", \"left\")\n",
    "      .groupBy(\"curr_category\")\n",
    "      .agg(F.sum(\"n\").alias(\"total_n\"))\n",
    "      .orderBy(F.desc(\"total_n\")))\n",
    "j2.explain(\"formatted\")\n",
    "\n",
    "# Save one plan for evidence\n",
    "with open(\"proof/plan_broadcast.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(j2._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_broadcast.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a908f0",
   "metadata": {},
   "source": [
    "## 4. Additional queries for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: daily GMV‑like metric (sum of n) for a specific title window\n",
    "q2_row = (row_df.filter((F.col(\"type\")==\"link\") & F.col(\"curr_title\").isin(\"Apache_Spark\",\"PySpark\"))\n",
    "           .groupBy(\"year\",\"month\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(\"year\",\"month\",\"curr_title\"))\n",
    "q2_col = (col_df.filter((F.col(\"type\")==\"link\") & F.col(\"curr_title\").isin(\"Apache_Spark\",\"PySpark\"))\n",
    "           .groupBy(\"year\",\"month\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(\"year\",\"month\",\"curr_title\"))\n",
    "\n",
    "# Trigger\n",
    "_ = q2_row.count(); _ = q2_col.count()\n",
    "\n",
    "# Q3: heavy cardinality grouping\n",
    "q3_row = row_df.groupBy(\"prev_title\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(F.desc(\"n\")).limit(100)\n",
    "q3_col = col_df.groupBy(\"prev_title\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(F.desc(\"n\")).limit(100)\n",
    "_ = q3_row.count(); _ = q3_col.count()\n",
    "\n",
    "print(\"Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab3_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98782e23",
   "metadata": {},
   "source": [
    "## 5. Save sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dee6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, pandas as pd\n",
    "pathlib.Path(\"outputs/lab3\").mkdir(parents=True, exist_ok=True)\n",
    "q1_row.limit(10).toPandas().to_csv(\"outputs/lab3/q1_row_top10.csv\", index=False)\n",
    "q1_col.limit(10).toPandas().to_csv(\"outputs/lab3/q1_col_top10.csv\", index=False)\n",
    "j2.limit(20).toPandas().to_csv(\"outputs/lab3/j2_broadcast_sample.csv\", index=False)\n",
    "print(\"Saved sample outputs in outputs/lab3/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a87a",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40778498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca52c5f-bc9f-46bd-acd4-ab7ceda78aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
